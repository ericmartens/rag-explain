{"cells": [{"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "# Source attribution detection for RAG based natural language question responses using watsonx"}, {"metadata": {"jupyter": {"outputs_hidden": false}, "pycharm": {"name": "#%% md\n"}, "id": "0ab8983f-b0aa-45ec-b797-e24394fbdd1e"}, "cell_type": "markdown", "source": "## Notebook content\nThis notebook contains the steps and code to demonstrate support of Retrieval Augumented Generation in watsonx.ai and identify source attribution. It introduces commands for data retrieval, knowledge base building & querying, and model testing. Some familiarity with Python is helpful. This notebook uses Python 3.10, and is based on [this notebook](https://github.com/IBM/watson-openscale-samples/blob/main/WatsonX.Governance/Cloud/GenAI/samples/source-attribution-using-protodash-for-rag-usecase%20.ipynb) by Sowmaya Kollipara.\n\n### About Retrieval Augmented Generation\nRetrieval Augmented Generation (RAG) is a versatile pattern that can unlock a number of use cases requiring factual recall of information, such as querying a knowledge base in natural language.\n\nIn its simplest form, RAG requires 3 steps:\n\n- Index knowledge base passages (once)\n- Retrieve relevant passage(s) from knowledge base (for every user query)\n- Generate a response by feeding retrieved passage into a large language model (for every user query)\n\n#### Source Attribution Detection\nSource attribution detection is to identify the part(s) from the context which could have attributed to the response from the foundation model . \n\n#### The flow of this notebook is as follows :\n1. Building a knowledge base\n2. Getting the relevant information from the vectordb to get the relevant context for a bunch of questions for which user is looking for responses.\n3. Construct the prompt using the question and relevant context for each question considered.\n4. Generate the retrieval augmented response to the question using the foundation models hosted on watsonx.ai\n5. Intialize the WOS client, supply the configuration needed for identifying source attribution.\n6. Identify the source attribution for the RAG based responses."}, {"metadata": {"jupyter": {"outputs_hidden": false}, "pycharm": {"name": "#%% md\n"}, "id": "57b75b55-a478-43c1-b6cf-55be93513e5f"}, "cell_type": "markdown", "source": "### Install and import the dependecies"}, {"metadata": {"jupyter": {"outputs_hidden": false}, "pycharm": {"name": "#%%\n"}, "id": "2c7ff07d-c92c-41ea-a395-74d81085b889"}, "cell_type": "code", "source": "!pip install \"langchain==0.0.345\" | tail -n 1\n!pip install wget | tail -n 1\n!pip install sentence-transformers | tail -n 1\n!pip install \"chromadb==0.3.26\" | tail -n 1\n!pip install \"ibm-watson-machine-learning>=1.0.335\" | tail -n 1\n!pip install \"pydantic==1.10.0\" | tail -n 1\n!pip install --upgrade ibm-metrics-plugin  --no-cache | tail -n 1\n!pip install --upgrade ibm-watson-openscale --no-cache | tail -n 1\n!pip install --upgrade pyspark==3.3.1 | tail -n 1\n!pip install -U \"torch==2.0.0\"\n", "execution_count": null, "outputs": []}, {"metadata": {"id": "69e1345c-ed06-49c4-88e7-f3febf661a0b"}, "cell_type": "markdown", "source": "### Edit the two values below with your API key and Project ID\n\nRefer to the  [lab guide](https://github.com/ericmartens/rag-explain/blob/main/RAG_Explanations_Lab_Guide.pdf) for instructions on gathering the correct values for API\\_KEY and PROJECT\\_ID. Copy and paste the values into the cell below in between the quotation marks."}, {"metadata": {"jupyter": {"outputs_hidden": false}, "pycharm": {"name": "#%%\n"}, "id": "a4797d28-f29e-4884-a1e2-ed98361da36a"}, "cell_type": "code", "source": "API_KEY = \"___PASTE API KEY HERE___\"\nPROJECT_ID = \"___PASTE PROJECT ID HERE___\"", "execution_count": null, "outputs": []}, {"metadata": {"id": "5c57f6ff-38e0-4d41-a974-3d49bebe0e7f"}, "cell_type": "markdown", "source": "## Foundation Models on `watsonx.ai`\n\nIBM watsonx foundation models are among the <a href=\"https://python.langchain.com/docs/integrations/llms/watsonxllm\" target=\"_blank\" rel=\"noopener no referrer\">list of LLM models supported by Langchain</a>. This example shows how to communicate with <a href=\"https://newsroom.ibm.com/2023-09-28-IBM-Announces-Availability-of-watsonx-Granite-Model-Series,-Client-Protections-for-IBM-watsonx-Models\" target=\"_blank\" rel=\"noopener no referrer\">Granite Model Series</a> using <a href=\"https://python.langchain.com/docs/get_started/introduction\" target=\"_blank\" rel=\"noopener no referrer\">Langchain</a>."}, {"metadata": {"id": "d06a167a-65b8-45a4-ba67-82230de5b9ac"}, "cell_type": "code", "source": "from ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes\n\nmodel_id = ModelTypes.GRANITE_13B_CHAT_V2", "execution_count": null, "outputs": []}, {"metadata": {"id": "0d506916-e286-4600-a023-a44efa978f7e"}, "cell_type": "markdown", "source": "### Defining the model parameters\nWe need to provide a set of model parameters that will influence the result."}, {"metadata": {"id": "fd268836-4af6-4555-a72b-5b9c8eb5c345"}, "cell_type": "code", "source": "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\nfrom ibm_watson_machine_learning.foundation_models.utils.enums import DecodingMethods\n\nparameters = {\n    GenParams.DECODING_METHOD: DecodingMethods.GREEDY,\n    GenParams.MIN_NEW_TOKENS: 1,\n    GenParams.MAX_NEW_TOKENS: 100,\n    GenParams.STOP_SEQUENCES: [\"<|endoftext|>\"]\n}", "execution_count": null, "outputs": []}, {"metadata": {"id": "bece9b63-0f34-48b5-bbef-6fa5f14c9838"}, "cell_type": "markdown", "source": "### LangChain CustomLLM wrapper for watsonx model\nInitialize the `WatsonxLLM` class from Langchain with defined parameters and `ibm/granite-13b-chat-v2`. "}, {"metadata": {"id": "8db8011d-2bb9-44ae-a75c-a05d1db28840"}, "cell_type": "code", "source": "from langchain.llms import WatsonxLLM\n\nwatsonx_granite = WatsonxLLM(\n    model_id=model_id.value,\n    url=\"https://us-south.ml.cloud.ibm.com\",\n    apikey=API_KEY,\n    project_id=PROJECT_ID,\n    params=parameters\n)", "execution_count": null, "outputs": []}, {"metadata": {"id": "52d9e7ee-8564-47fd-85eb-50a99f6f153c"}, "cell_type": "markdown", "source": "### Set up the OpenScale client\n\nExplanations use the watsonx.governance monitoring (OpenScale) service to calculate which of the context paragraphs contributed to the model's answer. The next cell uses the supplied credentials to authenticate with the OpenScale client."}, {"metadata": {"id": "852fbe8b-f0b1-4dde-b3c8-8e9ab4fb4950"}, "cell_type": "code", "source": "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator, BearerTokenAuthenticator\n\nfrom ibm_watson_openscale import *\nfrom ibm_watson_openscale.supporting_classes.enums import *\nfrom ibm_watson_openscale.supporting_classes import *\n\n\nauthenticator = IAMAuthenticator(apikey=API_KEY)\nclient = APIClient(authenticator=authenticator)\nclient.version", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Import libraries"}, {"metadata": {}, "cell_type": "code", "source": "import wget\nimport os\n\nfrom langchain.document_loaders import TextLoader\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.chains import RetrievalQA\nfrom langchain.schema.document import Document\n\nfrom ibm_metrics_plugin.common.utils.constants import ExplainabilityMetricType\nfrom ibm_metrics_plugin.metrics.explainability.entity.explain_config import ExplainConfig\nfrom ibm_metrics_plugin.common.utils.constants import InputDataType, ProblemType\n\nimport pandas as pd\nimport nltk\nimport warnings\nimport json", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Source attribution detection for RAG based response for LLMs\n\nSource Attrbution for RAG based response is computed using Protodash Explainer. The information needed for this computation:\n1. Response data for which source attribution has to be identified. This is considered as input data.\n2. Context information retained using RAG. This is considered as reference data.\n\nUsing the above information, prototypes of the input are identified. Using this technique the source, in the context which has contributed to the response is identified.\n\n## Explanation\n\nSource attribution can be understood using the weights (the attribution/contribution factor) and the prototypes (the relevant context/source) which has influenced the response by the foundation model behind the scenes. For example, a weight of 1.0 indicates that a single paragraph of the context informatino has contributed to the response by foundation model. Likewise, weights of three differing values indicate that three paragraphs have contributed to the response by foundation model behind the scenes. The prototype values are the paragraphs supplied as part of the relevant context."}, {"metadata": {}, "cell_type": "markdown", "source": "## Define the function to get the RAG response"}, {"metadata": {}, "cell_type": "code", "source": "def get_rag_response(rag_documents, rag_question_list):\n    text_splitter = CharacterTextSplitter(chunk_size=1200, chunk_overlap=0)\n    texts = text_splitter.split_documents(rag_documents)\n    \n    # Create an embedding function\n    embeddings = HuggingFaceEmbeddings()\n    docsearch = Chroma.from_documents(texts, embeddings)\n    \n    # Generate a retrieval-augmented response to a question\n    qa = RetrievalQA.from_chain_type(llm=watsonx_granite, chain_type=\"stuff\", retriever=docsearch.as_retriever())\n    \n    questions = rag_question_list\n        \n    responses = []\n    contexts = []\n    for query in questions:\n        #Retrive relevant context for each question from the vector db\n        docs = docsearch.as_retriever().get_relevant_documents(query)\n\n        context = []\n        #Extract the needed information\n        for doc in docs:\n            context.append(doc.to_json()['kwargs']['page_content'])\n\n        #Capture the context\n        contexts.append(context)\n\n        #Run the prompt and get the response\n        response = qa.run(query)\n        responses.append(response)\n    \n    #Print the result\n    for query in questions:\n        print(f\"{query} \\n {responses[questions.index(query)]} \\n\")\n        \n    data = pd.DataFrame({\"generated_text\":responses,\"context\":contexts})\n    data.head()\n    \n    return data\n    ", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Define the function to compute the explanations"}, {"metadata": {}, "cell_type": "code", "source": "def compute_protodash(rag_dataframe, rag_question_list):\n    embeddings = HuggingFaceEmbeddings()\n    # Update the configuration for source attribution\n    config_json = {\n            \"configuration\": {\n                \"input_data_type\": InputDataType.TEXT.value,\n                \"problem_type\": ProblemType.QA.value,\n                \"feature_columns\":[\"context\"],\n                \"prediction\": \"generated_text\", #Column name that has the prompt response from FM\n                \"context_column\": \"context\",\n                \"explainability\": {\n                    \"metrics_configuration\":{\n                        ExplainabilityMetricType.PROTODASH.value:{\n                                    \"embedding_fn\": embeddings.embed_documents #Make sure to supply the embedded function else TfIDfvectorizer will be used\n                                }\n                    }\n                }\n            }\n        }\n    warnings.filterwarnings(\"ignore\")\n    results = client.ai_metrics.compute_metrics(configuration=config_json,data_frame=rag_dataframe)\n    \n    metrics = results.get(\"metrics_result\")\n    results = metrics.get(\"explainability\").get(\"protodash\")\n    \n    for idx, entry in enumerate(results):\n        print(f\"====idx:{idx}: Question:{rag_question_list[idx]} Response:{rag_dataframe['generated_text'][idx]}====\")\n        print(json.dumps(entry,indent=4))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# USE CASE 1: Mafia History\n\nDownload the first document, a history of mafia involvement in Las Vegas, from a Las Vegas Review Journal article by Jeff German."}, {"metadata": {}, "cell_type": "code", "source": "# Get the file\nfilename = 'mafia_history.txt'\nurl = 'https://raw.githubusercontent.com/ericmartens/rag-explain/refs/heads/main/mafia_history.txt'\n\nif not os.path.isfile(filename):\n    wget.download(url, out=filename)\n\n# Build up the knowledge base\nloader = TextLoader(filename)\ndocuments = loader.load()\n\nquery0 = \"Why was Bugsy Seigel killed?\"\nquery1 = \"Which political figures have tried to curtail mob activities in Las Vegas?\"\nquery2 = \"What casinos were owned by the mob?\"\n\nquestion_list = [query0, query1, query2]\n\nrag_data = get_rag_response(documents, question_list)\ncompute_protodash(rag_data, question_list)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# USE CASE 2: The Super Bowl\n\nDownload the second document, a Wikipedia entry for the origins and overview of the Super Bowl."}, {"metadata": {}, "cell_type": "code", "source": "# Get the file\nfilename = 'super_bowl.txt'\nurl = 'https://raw.githubusercontent.com/ericmartens/rag-explain/refs/heads/main/super_bowl.txt'\n\nif not os.path.isfile(filename):\n    wget.download(url, out=filename)\n\n# Build up the knowledge base\nloader = TextLoader(filename)\ndocuments = loader.load()\n\nq0 = 'Who is the Lombardi Trophy named for?'\nq1 = 'Which teams have never appeared in a Super Bowl?'\nq2 = 'Where did the Super Bowl get its name?'\n\nquestion_list = [q0, q1, q2]\n\nrag_data = get_rag_response(documents, question_list)\ncompute_protodash(rag_data, question_list)", "execution_count": null, "outputs": []}, {"metadata": {"id": "cd6de6d8-ce53-4cb7-b49e-82d7a6c123f0"}, "cell_type": "markdown", "source": "# Custom content\n\nPlease refer to your lab guide for instructions on using custom text in the lab."}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {"id": "79e3a50a-d216-45ef-9188-2794a04684e8"}, "cell_type": "code", "source": "# Build up the knowledge base\ncustom_text = streaming_body_1.read().decode('utf-8', errors='ignore')\ntext_splitter = CharacterTextSplitter(chunk_size=1200, chunk_overlap=100)\ndocuments = [Document(page_content=x) for x in text_splitter.split_text(custom_text)]\n\nquery0 = \"Question text 1\"\nquery1 = \"Question text 2\"\nquery2 = \"Question text 3\"\n\nquestion_list = [query0, query1, query2]\n\nrag_data = get_rag_response(documents, question_list)\ncompute_protodash(rag_data, question_list)", "execution_count": null, "outputs": []}, {"metadata": {"id": "3182afa9-4fae-484a-b42b-915173e7d182"}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.10", "language": "python"}, "language_info": {"name": "python", "version": "3.10.14", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 4}