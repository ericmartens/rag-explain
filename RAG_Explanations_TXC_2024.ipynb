{
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.11",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Source attribution detection for RAG based natural language question responses using WatsonX ",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "attachments": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Edit the two values below with your API key and Project ID\n\nRefer to the  [lab guide](https://github.com/ericmartens/rag-explain/blob/main/RAG_Explanations_Lab_Guide.docx) for instructions on gathering the correct values for API\\_KEY and PROJECT\\_ID. Copy and paste the values into the cell below in between the quotation marks.",
   "metadata": {
    "id": "f4680074-b087-4b3d-a4ea-3a4bd357305d"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "API_KEY = \"___PASTE_YOUR_API_KEY_HERE_BETWEEN_THE_QUOTES___\"\n",
    "PROJECT_ID = \"___PASTE_YOUR_PROJECT_ID_HERE_BETWEEN_THE_QUOTES___\""
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "a4797d28-f29e-4884-a1e2-ed98361da36a",
    "msg_id": "e13d5b45-288b-4175-a064-de1b27dd16dc"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Notebook content\nThis notebook contains the steps and code to demonstrate support of Retrieval Augumented Generation in watsonx.ai and identify source attribution. It introduces commands for data retrieval, knowledge base building & querying, and model testing. Some familiarity with Python is helpful. This notebook uses Python 3.11, and is based on [this notebook](https://github.com/IBM/watson-openscale-samples/blob/main/WatsonX.Governance/Cloud/GenAI/samples/source-attribution-using-protodash-for-rag-usecase%20.ipynb) by Sowmaya Kollipara.\n\n### About Retrieval Augmented Generation\nRetrieval Augmented Generation (RAG) is a versatile pattern that can unlock a number of use cases requiring factual recall of information, such as querying a knowledge base in natural language.\n\nIn its simplest form, RAG requires 3 steps:\n\n- Index knowledge base passages (once)\n- Retrieve relevant passage(s) from knowledge base (for every user query)\n- Generate a response by feeding retrieved passage into a large language model (for every user query)\n\n#### Source Attribution Detection\nSource attribution detection is to identify the part(s) from the context which could have attributed to the response from the foundation model . \n\n#### The flow of this notebook is as follows :\n1. Building a knowledge base\n2. Getting the relevant information from the vectordb to get the relevant context for a bunch of questions for which user is looking for responses.\n3. Construct the prompt using the question and relevant context for each question considered.\n4. Generate the retrieval augmented response to the question using the foundation models hosted on watsonx.ai\n5. Intialize the WOS client, supply the configuration needed for identifying source attribution.\n6. Identify the source attribution for the RAG based responses.",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "5492522a-1dfa-489e-92ff-b9c534cd41ea"
   }
  },
  {
   "cell_type": "markdown",
   "source": "### Install and import the dependecies",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": "!pip install \"langchain==0.0.345\" | tail -n 1\n!pip install wget | tail -n 1\n!pip install sentence-transformers | tail -n 1\n!pip install \"chromadb==0.3.26\" | tail -n 1\n!pip install \"ibm-watson-machine-learning>=1.0.335\" | tail -n 1\n!pip install \"pydantic==1.10.0\" | tail -n 1\n!pip install --upgrade ibm-metrics-plugin  --no-cache | tail -n 1\n!pip install --upgrade ibm-watson-openscale --no-cache | tail -n 1\n!pip install --upgrade pyspark==3.3.1 | tail -n 1\n!pip install -U \"torch==2.0.0\"",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "310a971f-6b9f-4aa4-b90e-59885ec3eef1",
    "msg_id": "1420a19f-7c52-4aee-81bb-768aca7254c4"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Foundation Models on `watsonx.ai`\n\nIBM watsonx foundation models are among the <a href=\"https://python.langchain.com/docs/integrations/llms/watsonxllm\" target=\"_blank\" rel=\"noopener no referrer\">list of LLM models supported by Langchain</a>. This example shows how to communicate with <a href=\"https://newsroom.ibm.com/2023-09-28-IBM-Announces-Availability-of-watsonx-Granite-Model-Series,-Client-Protections-for-IBM-watsonx-Models\" target=\"_blank\" rel=\"noopener no referrer\">Granite Model Series</a> using <a href=\"https://python.langchain.com/docs/get_started/introduction\" target=\"_blank\" rel=\"noopener no referrer\">Langchain</a>.",
   "metadata": {
    "id": "5c57f6ff-38e0-4d41-a974-3d49bebe0e7f"
   }
  },
  {
   "cell_type": "code",
   "source": "from ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes\n\nmodel_id = ModelTypes.GRANITE_13B_CHAT_V2",
   "metadata": {
    "id": "d06a167a-65b8-45a4-ba67-82230de5b9ac",
    "msg_id": "e52bca30-3c5f-4162-a6d0-76a59288c76f"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Defining the model parameters\nWe need to provide a set of model parameters that will influence the result.",
   "metadata": {
    "id": "0d506916-e286-4600-a023-a44efa978f7e"
   }
  },
  {
   "cell_type": "code",
   "source": "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\nfrom ibm_watson_machine_learning.foundation_models.utils.enums import DecodingMethods\n\nparameters = {\n    GenParams.DECODING_METHOD: DecodingMethods.GREEDY,\n    GenParams.MIN_NEW_TOKENS: 1,\n    GenParams.MAX_NEW_TOKENS: 100,\n    GenParams.STOP_SEQUENCES: [\"<|endoftext|>\"]\n}",
   "metadata": {
    "id": "fd268836-4af6-4555-a72b-5b9c8eb5c345",
    "msg_id": "f1c5446b-59a3-487e-990b-bb831ab1344c"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### LangChain CustomLLM wrapper for watsonx model\nInitialize the `WatsonxLLM` class from Langchain with defined parameters and `ibm/granite-13b-chat-v2`. ",
   "metadata": {
    "id": "bece9b63-0f34-48b5-bbef-6fa5f14c9838"
   }
  },
  {
   "cell_type": "code",
   "source": "from langchain.llms import WatsonxLLM\n\nwatsonx_granite = WatsonxLLM(\n    model_id=model_id.value,\n    url=\"https://us-south.ml.cloud.ibm.com\",\n    apikey=API_KEY,\n    project_id=project_id,\n    params=parameters\n)",
   "metadata": {
    "id": "8db8011d-2bb9-44ae-a75c-a05d1db28840",
    "msg_id": "8c279202-f20c-4af4-bbd4-7266f0674be8"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Generate a retrieval-augmented response to a question\nBuild the `RetrievalQA` (question answering chain) to automate the RAG task.",
   "metadata": {
    "id": "37837457-cce6-4a40-9e5b-c6a936fdeaa1"
   }
  },
  {
   "cell_type": "code",
   "source": "from langchain.chains import RetrievalQA\n\nqa = RetrievalQA.from_chain_type(llm=watsonx_granite, chain_type=\"stuff\", retriever=docsearch.as_retriever())",
   "metadata": {
    "id": "c9fc5fea-df3f-4bcf-b695-d8885f15c250",
    "msg_id": "dc2adeab-9574-4811-9308-cf1f82b05ece"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Set up openscale client",
   "metadata": {
    "id": "52d9e7ee-8564-47fd-85eb-50a99f6f153c"
   }
  },
  {
   "cell_type": "code",
   "source": "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator, BearerTokenAuthenticator\n\nfrom ibm_watson_openscale import *\nfrom ibm_watson_openscale.supporting_classes.enums import *\nfrom ibm_watson_openscale.supporting_classes import *\n\n\nauthenticator = IAMAuthenticator(apikey=API_KEY)\nclient = APIClient(authenticator=authenticator)\nclient.version",
   "metadata": {
    "id": "852fbe8b-f0b1-4dde-b3c8-8e9ab4fb4950",
    "msg_id": "705487d8-4017-4e2d-a289-c15d18669092"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Update the configuration needed for source attribution",
   "metadata": {
    "id": "f0672e61-0b9e-4c72-a7b4-54458af2e833"
   }
  },
  {
   "cell_type": "code",
   "source": "from ibm_metrics_plugin.common.utils.constants import ExplainabilityMetricType\nfrom ibm_metrics_plugin.metrics.explainability.entity.explain_config import ExplainConfig\nfrom ibm_metrics_plugin.common.utils.constants import InputDataType, ProblemType\n\nconfig_json = {\n            \"configuration\": {\n                \"input_data_type\": InputDataType.TEXT.value,\n                \"problem_type\": ProblemType.QA.value,\n                \"feature_columns\":[\"context\"],\n                \"prediction\": \"generated_text\", #Column name that has the prompt response from FM\n                \"context_column\": \"context\",\n                \"explainability\": {\n                    \"metrics_configuration\":{\n                        ExplainabilityMetricType.PROTODASH.value:{\n                                    \"embedding_fn\": embeddings.embed_documents #Make sure to supply the embedded function else TfIDfvectorizer will be used\n                                }\n                    }\n                }\n            }\n        }",
   "metadata": {
    "id": "59fbd6a6-a0a6-4f8c-8928-85c31bd8f3b1",
    "msg_id": "a426825f-7bc8-4ac1-aad1-27fdb94d5a8d"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "id": "165be5f7-60ec-461d-b187-b11d5acf659b"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "id": "d3ece5e0-626e-4751-91c6-a6936f997fd8"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "id": "aba2c491-e53a-4f7a-8baf-2979afed19f0"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "id": "6a6dcfb5-0503-4e71-99d8-ce802518c8b0"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "id": "16688a8d-2c6d-455d-a5e8-0d3fe4b7e4f5"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "id": "fa912e1a-003a-41c5-a8e8-c1c75be18a47"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "id": "c864e168-64b6-488f-bf89-261c4cd93765"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "id": "53132574-ba0b-43b8-b649-cd7510f5f973"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "id": "e3c50b66-79c8-4c5d-b0f1-d5008d216935"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "id": "97e7d16e-823e-4bcf-88c4-4fa783d76f36"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "id": "7f51b570-b9c3-488d-b088-79cf3e1a6239"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "<a id=\"data\"></a>\n## Document data loading\n\nDownload the file with State of the Union.",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": "import wget\nimport os\n\nfilename = 'state_of_the_union.txt'\nurl = 'https://raw.github.com/IBM/watson-machine-learning-samples/master/cloud/data/foundation_models/state_of_the_union.txt'\n\nif not os.path.isfile(filename):\n    wget.download(url, out=filename)",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "92955404-fdf5-4c46-9b2e-720704cdf384",
    "msg_id": "87fa2c00-ad08-4963-8cf3-623329fd377c"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "<a id=\"build_base\"></a>\n## Build up knowledge base\n\nThe most common approach in RAG is to create dense vector representations of the knowledge base in order to calculate the semantic similarity to a given user query.\n\nIn this basic example, we take the State of the Union speech content (filename), split it into chunks, embed it using an open-source embedding model, load it into <a href=\"https://www.trychroma.com/\" target=\"_blank\" rel=\"noopener no referrer\">Chroma</a>, and then query it.",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": "from langchain.document_loaders import TextLoader\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores import Chroma\n\nloader = TextLoader(filename)\ndocuments = loader.load()\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\ntexts = text_splitter.split_documents(documents)",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "6d94feb2-7821-406c-9515-7ee8d6e9d132",
    "msg_id": "5fda7667-4814-47cb-819b-eabfe904bbdb"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "The dataset we are using is already split into self-contained passages that can be ingested by Chroma.",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": "### Create an embedding function\n\nNote that you can feed a custom embedding function to be used by chromadb. The performance of Chroma db may differ depending on the embedding model used.",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": "from langchain.embeddings import HuggingFaceEmbeddings\n\nembeddings = HuggingFaceEmbeddings()\ndocsearch = Chroma.from_documents(texts, embeddings)",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "885d56e8-e129-42aa-90dc-ae4fc0be3d12",
    "msg_id": "57de1eac-45a1-44d8-af16-f33071a0f511"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "query0 = \"What did the president say about Ketanji Brown Jackson?\"\nquery1 = \"What is ARPA-H?\"\nquery2 = \"How much does it cost to make  a vial of Insulin?\"\nquery3 = \"What is the investment of Ford and GM to build electric vehicles?\"\nquery4 = \"What is the proposed tax rate for corporations?\"\nquery5 = \"What did president say about Bipartisan Infrastructure Act\"\nquery6 = \"What is Intel going to build?\"\nquery7 = \"How many new manufacturing jobs are created last year?\"\nquery8 = \"What did the president say about cancer death rate?\"\nquery9 = \"What are the dangers faced by troops in Iraq and Afganistan?\"\nquery10 = \"How many electric vehicle charging stations are built?\"\n\nquestions_list = [query0, query1 , query2, query3,query4, query5, query6, query7, query8, query9,query10]",
   "metadata": {
    "id": "0d974763-396b-4717-be9b-7ed56429746f",
    "msg_id": "0b6fe900-3de1-4e4d-babb-b19b8279b6a1"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Select questions\n\nGet questions from the previously loaded test dataset and retain the context for each question.",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": "#Select the question from the question list above .\nquestions = questions_list[0:2]\nfor question in questions:\n    print(question)\n",
   "metadata": {
    "id": "4d875399-5b86-46eb-91ea-d32f87999cd8",
    "msg_id": "ca768381-9fbb-43da-a104-1b8b1dd812e3"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Generate a retrieval-augmented response to a question",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "responses = []\ncontexts = []\nfor query in questions:\n    #Retrive relevant context for each question from the vector db\n    docs = docsearch.as_retriever().get_relevant_documents(query)\n\n    context = []\n    #Extaract the needed information\n    for doc in docs:\n        context.append(doc.to_json()['kwargs']['page_content'])\n\n    #Capture the context\n    contexts.append(context)\n\n    #Run the prompt and get the response\n    response = qa.run(query)\n    responses.append(response)\n    ",
   "metadata": {
    "id": "abf08219-9b3e-4120-b71b-bd6172e76a6c",
    "msg_id": "656c0c9a-ea5b-4627-aecb-e47f4b159027"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "#Print a sample context retrieved for a query \nprint(f\"Question:{questions[0]}\\n context:{contexts[0]}\")",
   "metadata": {
    "id": "81e53cee-089e-4e93-877c-dbf834b4a7c0",
    "msg_id": "e8ba8a2d-9d0c-4a98-b5c0-2bd7e0af0da0"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "#Print the result\nfor query in questions:\n    print(f\"{query} \\n {responses[questions.index(query)]} \\n\")",
   "metadata": {
    "id": "5013c229-cbe7-4fe5-9027-622426d59b97",
    "msg_id": "f2a9ec11-a357-4913-afe2-8d143d7c06c8"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "<a id=\"sourceattribution\"></a>\n### Source attribution detection for RAG based response for LLMs\n\nSource Attrbution for RAG based response is computed using Protodash Explainer . The information needed for this computation :\n1. Response data for which source attribution has to be identified. This is considered as input data.\n2. Context information retained using RAG . This is considered as reference data\n\nUsing the above information , prototypes of the input are identified . Using this technique the source in the context which has attributed to the response is identified.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Construct a dataframe with results , contexts to supply for source attribution\n- generated_text : Response from the foundation model\n- context: Relevant context retreived from vector db (chromadb in this example) for each question . For this notebook 5 questions are been considered for source attribution",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import pandas as pd\ndata = pd.DataFrame({\"generated_text\":responses,\"context\":contexts})\ndata.head()",
   "metadata": {
    "id": "4b93c5fa-f52f-4d80-b627-d39312956d50",
    "msg_id": "6ea8dd3e-7284-407f-b9eb-fb0126a4f371"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Run protodash explainer to identify source attribution for the RAG based responses",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import warnings\n\nwarnings.filterwarnings(\"ignore\")\nresults = client.ai_metrics.compute_metrics(configuration=config_json,data_frame=data)",
   "metadata": {
    "id": "43520a23-e579-4ff3-a0bf-49e707f6e27c",
    "msg_id": "1fd983b0-1594-4fd6-aa27-b11964926224"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "metrics = results.get(\"metrics_result\")\nresults = metrics.get(\"explainability\").get(\"protodash\")",
   "metadata": {
    "id": "076af503-b953-4e99-9cbc-c4c4f3e8d436",
    "msg_id": "49ff9453-d9ea-404b-b128-c5b8b7bd5c84"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import json\nfor idx, entry in enumerate(results):\n    print(f\"====idx:{idx}: Question:{questions[idx]} Response:{data['generated_text'][idx]}====\")\n    print(json.dumps(entry,indent=4))",
   "metadata": {
    "id": "3b8569a0-9aa8-42b8-b3e1-3da8cd683d82",
    "msg_id": "371d2bd8-a976-4913-8f04-915830676cc7"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Explanation\nSource attribution can be understood using the weights ( the attribution/contribution factor) and the prototypes ( the relevant context/source) which has attributed to the response by the foundation model behind the scenes . For example a weight: 1.0 indicate that that a single paragraph of the context has attributed for response by foundation model. Likewise weights : 0.6,0.3,0.1 indicate that 3  paragraphs have attributed for response by foundation model behind the scenes.   The prototype values are the paragraphs supplied as part of the relevant context . ",
   "metadata": {
    "id": "48e8b281-7c4d-4310-aa03-bd50288aaceb"
   }
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "id": "dcb17d7e-3e27-47e9-8270-b66e973a0657"
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
